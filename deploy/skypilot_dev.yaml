# SkyPilot configuration for Arbor GRPO training
#
# Quick start:
#   1. Set your training script name below (default: multihop_dev.py)
#   2. Edit the S3 checkpoint bucket below (line ~25) to point to your bucket
#   3. Launch: sky jobs launch -n training-job deploy/skypilot_dev.yaml --infra <aws|gcp|etc>
#   4. Monitor: sky logs training-job
#
# See deploy/README.md for detailed instructions and customization options.

name: arbor-training

resources:
  accelerators: L40s:4  # Uses 4x L40S GPUs (adjust based on your script's needs)
  use_spot: false       # Set to true for cheaper spot instances
  disk_size: 256        # GB of disk space

file_mounts:
  # Mount your S3 checkpoint bucket (required - edit the line below with your bucket)
  /mnt/checkpoints:
    source: s3://arbor-multihop-checkpoints/ # Change this to your S3 bucket path
    mode: MOUNT_CACHED
  # Mount DeepSpeed config (used if your script uses accelerate launch)
  /configs: ./configs 

workdir: .

setup: |
  echo "Begin setup..."
  curl -LsSf https://astral.sh/uv/install.sh | sh
  export PATH="$HOME/.local/bin:$PATH"
  uv sync
  
  echo "Installing common training dependencies..."
  uv pip install datasets
  uv pip install deepspeed accelerate
  
  # Script-specific dependencies (customize as needed)
  # Uncomment and add any additional packages your script requires:
  uv pip install ujson
  uv pip install bm25s
  uv pip install PyStemmer
  uv pip install "jax[cpu]"
  
  # Script-specific data downloads (customize as needed)
  # For multihop_dev.py, download and extract wiki abstracts
  # Extract to a temp location first
  curl -Ls https://huggingface.co/dspy/cache/resolve/main/wiki.abstracts.2017.tar.gz | tar -xz -C /tmp
  # Move to root directory where the script expects it (script runs from root)
  if [ -f /tmp/wiki.abstracts.2017.jsonl ]; then
    mv /tmp/wiki.abstracts.2017.jsonl .
  elif [ -f /tmp/wiki.abstracts.2017/wiki.abstracts.2017.jsonl ]; then
    mv /tmp/wiki.abstracts.2017/wiki.abstracts.2017.jsonl .
  fi
  
  echo "Setup complete!"

run: |
  echo "Begin training run..."
  
  # Set your training script name here (or pass via --env TRAINING_SCRIPT=your_script.py)
  # Scripts in examples/ directory: examples/your_script.py
  # Scripts in root directory: your_script.py
  TRAINING_SCRIPT=${TRAINING_SCRIPT:-examples/multihop_dev.py}
  
  # Use accelerate launch if USE_ACCELERATE=true is set (default for multihop_dev.py)
  # For other scripts, set USE_ACCELERATE=true if you need distributed training
  if [ "$USE_ACCELERATE" = "true" ] || [ "$TRAINING_SCRIPT" = "examples/multihop_dev.py" ]; then
    echo "Launching with accelerate and DeepSpeed..."
    uv run accelerate launch \
      --config_file /configs/deepspeed_config.yaml \
      --num_machines 1 \
      --num_processes ${SKYPILOT_NUM_GPUS_PER_NODE:-4} \
      "$TRAINING_SCRIPT"
  else
    echo "Launching script directly..."
    uv run python "$TRAINING_SCRIPT"
  fi
  
  echo "Training complete!"