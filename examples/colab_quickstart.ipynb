{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-header",
   "metadata": {
    "id": "colab-header"
   },
   "source": [
    "# üå≥ Arbor in Google Colab - Quick Start\n",
    "\n",
    "This notebook demonstrates how to use Arbor in Google Colab for fine-tuning language models with a Ray-like interface.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ziems/arbor/blob/main/examples/colab_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "First, let's install Arbor using `uv` for faster package management. This may take a few minutes as it installs the ML dependencies, but uv is significantly faster than pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-arbor",
   "metadata": {
    "id": "install-arbor"
   },
   "outputs": [],
   "source": [
    "# Install uv for faster package management\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Install Arbor from the colab-support branch (includes new Colab integration)\n",
    "!uv pip install --system git+https://github.com/Ziems/arbor.git@colab-support openai\n",
    "\n",
    "# Optional: Install flash attention for faster inference (takes ~15 min)\n",
    "# !uv pip install --system flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialization",
   "metadata": {
    "id": "initialization"
   },
   "source": [
    "## üöÄ Start Arbor Server\n",
    "\n",
    "Start Arbor in the background with a Ray-like interface. This will:\n",
    "- Auto-detect available GPUs\n",
    "- Create a config file automatically\n",
    "- Start the server in a background thread\n",
    "- Provide an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-arbor",
   "metadata": {
    "id": "init-arbor"
   },
   "outputs": [],
   "source": [
    "import arbor\n",
    "\n",
    "# Start Arbor (Ray-like interface)\n",
    "server_info = arbor.init()\n",
    "print(f\"Server running at: {server_info['base_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-setup",
   "metadata": {
    "id": "client-setup"
   },
   "source": [
    "## üîå Connect with OpenAI Client\n",
    "\n",
    "Use the standard OpenAI Python client to interact with Arbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-client",
   "metadata": {
    "id": "setup-client"
   },
   "outputs": [],
   "source": [
    "# Option 1: Use the convenience method\n",
    "client = arbor.get_client()\n",
    "\n",
    "# Option 2: Manual setup\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=server_info['base_url'],\n",
    "#     api_key=\"not-needed\"  # Local server doesn't need API key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-inference",
   "metadata": {
    "id": "test-inference"
   },
   "source": [
    "## üí¨ Test Inference\n",
    "\n",
    "Let's test inference with a small model to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-chat",
   "metadata": {
    "id": "test-chat"
   },
   "outputs": [],
   "source": [
    "# Test chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\",  # Small model for testing\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Tell me a joke about machine learning.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-data",
   "metadata": {
    "id": "upload-data"
   },
   "source": [
    "## üìÅ Upload Training Data\n",
    "\n",
    "Let's create some sample training data and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-data",
   "metadata": {
    "id": "create-data"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create sample SFT training data\n",
    "sample_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explain neural networks simply.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process information and learn patterns from data.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Deep learning is a type of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "    temp_file = f.name\n",
    "\n",
    "print(f\"Created training data: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-file",
   "metadata": {
    "id": "upload-file"
   },
   "outputs": [],
   "source": [
    "# Upload the training file\n",
    "with open(temp_file, 'rb') as f:\n",
    "    file_response = client.files.create(\n",
    "        file=f,\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "\n",
    "print(f\"üìÅ File uploaded: {file_response.id}\")\n",
    "training_file_id = file_response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-training",
   "metadata": {
    "id": "start-training"
   },
   "source": [
    "## üéØ Start Fine-tuning Job\n",
    "\n",
    "Now let's start a supervised fine-tuning (SFT) job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-job",
   "metadata": {
    "id": "create-job"
   },
   "outputs": [],
   "source": [
    "# Create fine-tuning job\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    method={\"type\": \"sft\"},\n",
    "    training_file=training_file_id,\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\"  # Small model for quick training\n",
    ")\n",
    "\n",
    "print(f\"üéØ Training job created: {job.id}\")\n",
    "print(f\"Status: {job.status}\")\n",
    "job_id = job.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-training",
   "metadata": {
    "id": "monitor-training"
   },
   "source": [
    "## üìä Monitor Training Progress with Real-time Logs\n",
    "\n",
    "Arbor now provides built-in log monitoring! You can watch training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-status",
   "metadata": {
    "id": "check-status"
   },
   "outputs": [],
   "source": [
    "# Option 1: Watch job with real-time logs (recommended!)\n",
    "print(\"üé¨ Starting real-time job monitoring...\")\n",
    "arbor.watch_job(job_id, max_time=300, show_logs=True)  # Watch for 5 minutes max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nv22707rj4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Traditional polling approach\n",
    "import time\n",
    "\n",
    "# Check job status manually\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(f\"üìä Job Status: {job_status.status}\")\n",
    "\n",
    "# Monitor until completion (with timeout)\n",
    "timeout = 300  # 5 minutes\n",
    "start_time = time.time()\n",
    "\n",
    "while job_status.status in ['queued', 'running'] and (time.time() - start_time) < timeout:\n",
    "    time.sleep(10)  # Check every 10 seconds\n",
    "    job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    print(f\"‚è±Ô∏è  [{elapsed}s] Status: {job_status.status}\")\n",
    "\n",
    "print(f\"\\nüéâ Final Status: {job_status.status}\")\n",
    "if job_status.fine_tuned_model:\n",
    "    print(f\"‚úÖ Fine-tuned model: {job_status.fine_tuned_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vbh7habr84j",
   "metadata": {},
   "source": [
    "### üÜö Monitoring Options\n",
    "\n",
    "**Option 1: `arbor.watch_job()`** - Shows real-time training logs as they happen ‚ú®  \n",
    "**Option 2: Traditional polling** - Just checks status periodically\n",
    "\n",
    "The new `watch_job()` function gives you much better visibility into what's happening during training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-finetuned",
   "metadata": {
    "id": "test-finetuned"
   },
   "source": [
    "## üß™ Test Fine-tuned Model\n",
    "\n",
    "If training completed successfully, let's test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-finetuned-model",
   "metadata": {
    "id": "test-finetuned-model"
   },
   "outputs": [],
   "source": [
    "if job_status.status == 'succeeded' and job_status.fine_tuned_model:\n",
    "    # Test the fine-tuned model\n",
    "    response = client.chat.completions.create(\n",
    "        model=job_status.fine_tuned_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is reinforcement learning?\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(\"üé≠ Fine-tuned model response:\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Training not completed yet. Status: {job_status.status}\")\n",
    "    if job_status.status == 'failed':\n",
    "        print(\"‚ùå Training failed. Check the logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-example",
   "metadata": {
    "id": "advanced-example"
   },
   "source": [
    "## üöÄ Advanced: DSPy GRPO with GPU Memory Sharing\n",
    "\n",
    "DSPy's GRPO optimizer can now use GPU memory sharing! This allows both inference and training to run on the same GPU, perfect for Colab's single GPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-example",
   "metadata": {
    "id": "grpo-example"
   },
   "outputs": [],
   "source": [
    "# DSPy GRPO with GPU Memory Sharing Example\n",
    "import dspy\n",
    "import random\n",
    "\n",
    "# Sample classification data (banking domain)\n",
    "sample_classification_data = [\n",
    "    dspy.Example(text=\"I want to transfer money to my friend\", label=\"transfer\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"What is my account balance?\", label=\"balance\").with_inputs(\"text\"), \n",
    "    dspy.Example(text=\"I lost my credit card\", label=\"card_issues\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"How do I reset my PIN?\", label=\"pin_change\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"When will my salary be deposited?\", label=\"direct_deposit\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"I need to pay my bills\", label=\"bill_payment\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"Can I increase my credit limit?\", label=\"credit_limit\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"Where is the nearest ATM?\", label=\"atm_location\").with_inputs(\"text\"),\n",
    "]\n",
    "\n",
    "# Create train and validation sets\n",
    "random.Random(42).shuffle(sample_classification_data)\n",
    "trainset = sample_classification_data[:6]\n",
    "valset = sample_classification_data[6:]\n",
    "\n",
    "print(f\"üìä Created training set: {len(trainset)} examples\")\n",
    "print(f\"üìä Created validation set: {len(valset)} examples\")\n",
    "\n",
    "# Define the classification task\n",
    "CLASSES = [\"transfer\", \"balance\", \"card_issues\", \"pin_change\", \"direct_deposit\", \"bill_payment\", \"credit_limit\", \"atm_location\"]\n",
    "classify = dspy.ChainOfThought(f\"text -> label: Literal{CLASSES}\")\n",
    "\n",
    "# Set up DSPy with Arbor provider\n",
    "from dspy.clients.lm_local_arbor import ArborProvider\n",
    "\n",
    "provider = ArborProvider()\n",
    "student_lm = dspy.LM(\n",
    "    model=f\"openai/arbor:HuggingFaceTB/SmolLM2-135M-Instruct\",  # Use the same small model\n",
    "    provider=provider, \n",
    "    temperature=0.7, \n",
    "    api_base=server_info['base_url'], \n",
    "    api_key=\"arbor\"\n",
    ")\n",
    "\n",
    "# Create student classifier\n",
    "student_classify = classify.deepcopy()\n",
    "student_classify.set_lm(student_lm)\n",
    "\n",
    "# Define evaluation metric\n",
    "metric = (lambda x, y, trace=None: x.label == y.label)\n",
    "\n",
    "print(f\"üéØ Classification task: {len(CLASSES)} classes\")\n",
    "print(\"üîß DSPy classifier set up with Arbor backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3yot6mruc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GRPO optimizer with GPU memory sharing\n",
    "from dspy.teleprompt.grpo import GRPO\n",
    "from dspy.clients.utils_finetune import SingleGPUConfig\n",
    "\n",
    "# Training configuration with new structured GPU config\n",
    "train_kwargs = {\n",
    "    'per_device_train_batch_size': 2,  # Smaller batch for Colab\n",
    "    'temperature': 0.7,\n",
    "    'beta': 0.02,\n",
    "    'learning_rate': 1e-5,\n",
    "    'gradient_checkpointing': False,\n",
    "    'bf16': True,\n",
    "    'lr_scheduler_type': 'constant_with_warmup',\n",
    "    'max_prompt_length': None,\n",
    "    'max_completion_length': None,\n",
    "    'lora': False,\n",
    "    'report_to': 'none',\n",
    "    'log_completions': False,\n",
    "}\n",
    "\n",
    "# Create GRPO compiler\n",
    "compiler = GRPO(\n",
    "    metric=metric,\n",
    "    multitask=True,\n",
    "    num_dspy_examples_per_grpo_step=2,  # Smaller for Colab\n",
    "    num_rollouts_per_grpo_step=2,       # Smaller for Colab\n",
    "    exclude_demos=True,\n",
    "    num_train_steps=10,                  # Fewer steps for demo\n",
    "    num_threads=4,\n",
    "    use_train_as_val=False,\n",
    "    num_steps_for_val=5,\n",
    "    train_kwargs=train_kwargs,\n",
    "    gpu_config=SingleGPUConfig(shared_memory=True)\n",
    ")\n",
    "\n",
    "print(\"üöÄ GRPO compiler configured with new structured GPU config\")\n",
    "print(\"   - Type: single GPU with shared memory\")\n",
    "print(\"   - VLLM inference: ~45% GPU memory\") \n",
    "print(\"   - Training: ~50% GPU memory\")\n",
    "print(\"   - Perfect for single GPU environments like Colab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ikidoy0z3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GRPO optimization (this will take several minutes)\n",
    "print(\"üèÉ Starting GRPO optimization...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. Launch VLLM inference server with 45% GPU memory\")\n",
    "print(\"  2. Start training process sharing the remaining GPU memory\")\n",
    "print(\"  3. Optimize the classifier through reinforcement learning\")\n",
    "print(\"\")\n",
    "\n",
    "# Compile the classifier with GRPO\n",
    "classify_optimized = compiler.compile(\n",
    "    student=student_classify,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GRPO optimization completed!\")\n",
    "print(\"üìà Your classifier has been optimized using reinforcement learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1uwjcx4cqc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the optimized classifier\n",
    "print(\"üß™ Evaluating GRPO-optimized classifier...\")\n",
    "\n",
    "# Test on validation set\n",
    "correct = 0\n",
    "total = len(valset)\n",
    "\n",
    "for example in valset:\n",
    "    try:\n",
    "        prediction = classify_optimized(text=example.text)\n",
    "        if prediction.label == example.label:\n",
    "            correct += 1\n",
    "        print(f\"Text: '{example.text}'\")\n",
    "        print(f\"  Expected: {example.label}\")\n",
    "        print(f\"  Predicted: {prediction.label}\")\n",
    "        print(f\"  ‚úÖ Correct\" if prediction.label == example.label else \"  ‚ùå Wrong\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        total -= 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"üìä GRPO Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "\n",
    "# Compare with original classifier on a sample\n",
    "print(\"\\nüîÑ Comparing with original classifier on a test example:\")\n",
    "test_text = \"I want to check my account balance\"\n",
    "print(f\"Test: '{test_text}'\")\n",
    "\n",
    "try:\n",
    "    original_pred = student_classify(text=test_text)\n",
    "    optimized_pred = classify_optimized(text=test_text)\n",
    "    \n",
    "    print(f\"Original prediction: {original_pred.label}\")\n",
    "    print(f\"GRPO-optimized prediction: {optimized_pred.label}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in comparison: {e}\")\n",
    "\n",
    "print(\"\\nüéâ GPU memory sharing with DSPy GRPO is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server-management",
   "metadata": {
    "id": "server-management"
   },
   "source": [
    "## üõ†Ô∏è Server Management\n",
    "\n",
    "Arbor provides utilities to check server status and manage the background process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server-status",
   "metadata": {
    "id": "server-status"
   },
   "outputs": [],
   "source": [
    "# Check server status\n",
    "status = arbor.status()\n",
    "if status:\n",
    "    print(\"üå≥ Arbor server is running:\")\n",
    "    print(f\"   Host: {status['host']}\")\n",
    "    print(f\"   Port: {status['port']}\")\n",
    "    print(f\"   Base URL: {status['base_url']}\")\n",
    "    print(f\"   Storage: {status['storage_path']}\")\n",
    "    print(f\"   GPU IDs: {status['gpu_ids']}\")\n",
    "else:\n",
    "    print(\"‚ùå No Arbor server running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## üßπ Cleanup\n",
    "\n",
    "When you're done, you can shut down the Arbor server (optional - it will auto-cleanup when the notebook ends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown",
   "metadata": {
    "id": "shutdown"
   },
   "outputs": [],
   "source": [
    "# Shutdown the server (optional)\n",
    "# arbor.shutdown()\n",
    "\n",
    "# Clean up temporary file\n",
    "if 'temp_file' in locals():\n",
    "    os.unlink(temp_file)\n",
    "    print(\"üóëÔ∏è  Cleaned up temporary training file\")\n",
    "\n",
    "print(\"‚úÖ Session cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that you've got Arbor running in Colab, here are some things to try:\n",
    "\n",
    "1. **Experiment with different models**: Try larger models like `Qwen/Qwen2-0.5B-Instruct`\n",
    "2. **Use your own data**: Upload your own JSONL training files\n",
    "3. **Try GRPO training**: Uncomment the GRPO example above\n",
    "4. **Integrate with DSPy**: Use Arbor as a backend for DSPy programs\n",
    "5. **Monitor training**: Use the monitoring endpoints to track GPU usage and training metrics\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Arbor GitHub Repository](https://github.com/Ziems/arbor)\n",
    "- [DSPy RL Optimization Examples](https://dspy.ai/tutorials/rl_papillon/)\n",
    "- [DSPy Discord Community](https://discord.gg/ZAEGgxjPUe)\n",
    "\n",
    "### üí° Pro Tips for Colab\n",
    "\n",
    "- Use `arbor.init()` at the start of your notebook - it's designed to be safe to call multiple times\n",
    "- The server will auto-cleanup when your Colab session ends\n",
    "- For long-running training jobs, consider upgrading to Colab Pro for more stable GPU access\n",
    "- Monitor your GPU usage to avoid hitting Colab limits\n",
    "\n",
    "Happy fine-tuning! üå≥‚ú®"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
