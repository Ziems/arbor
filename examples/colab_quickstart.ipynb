{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-header",
   "metadata": {
    "id": "colab-header"
   },
   "source": [
    "# üå≥ Arbor in Google Colab - Quick Start\n",
    "\n",
    "This notebook demonstrates how to use Arbor in Google Colab for fine-tuning language models with a Ray-like interface.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ziems/arbor/blob/main/examples/colab_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "First, let's install Arbor using `uv` for faster package management. This may take a few minutes as it installs the ML dependencies, but uv is significantly faster than pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-arbor",
   "metadata": {
    "id": "install-arbor"
   },
   "outputs": [],
   "source": [
    "# Install uv for faster package management\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Install Arbor from the colab-support branch (includes new Colab integration)\n",
    "!uv pip install --system git+https://github.com/Ziems/arbor.git@colab-support openai\n",
    "\n",
    "# Optional: Install flash attention for faster inference (takes ~15 min)\n",
    "# !uv pip install --system flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialization",
   "metadata": {
    "id": "initialization"
   },
   "source": [
    "## üöÄ Start Arbor Server\n",
    "\n",
    "Start Arbor in the background with a Ray-like interface. This will:\n",
    "- Auto-detect available GPUs\n",
    "- Create a config file automatically\n",
    "- Start the server in a background thread\n",
    "- Provide an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-arbor",
   "metadata": {
    "id": "init-arbor"
   },
   "outputs": [],
   "source": [
    "import arbor\n",
    "\n",
    "# Start Arbor (Ray-like interface)\n",
    "server_info = arbor.init()\n",
    "print(f\"Server running at: {server_info['base_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-setup",
   "metadata": {
    "id": "client-setup"
   },
   "source": [
    "## üîå Connect with OpenAI Client\n",
    "\n",
    "Use the standard OpenAI Python client to interact with Arbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-client",
   "metadata": {
    "id": "setup-client"
   },
   "outputs": [],
   "source": [
    "# Option 1: Use the convenience method\n",
    "client = arbor.get_client()\n",
    "\n",
    "# Option 2: Manual setup\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=server_info['base_url'],\n",
    "#     api_key=\"not-needed\"  # Local server doesn't need API key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-inference",
   "metadata": {
    "id": "test-inference"
   },
   "source": [
    "## üí¨ Test Inference\n",
    "\n",
    "Let's test inference with a small model to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-chat",
   "metadata": {
    "id": "test-chat"
   },
   "outputs": [],
   "source": [
    "# Test chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\",  # Small model for testing\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Tell me a joke about machine learning.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-data",
   "metadata": {
    "id": "upload-data"
   },
   "source": [
    "## üìÅ Upload Training Data\n",
    "\n",
    "Let's create some sample training data and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-data",
   "metadata": {
    "id": "create-data"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create sample SFT training data\n",
    "sample_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explain neural networks simply.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process information and learn patterns from data.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Deep learning is a type of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "    temp_file = f.name\n",
    "\n",
    "print(f\"Created training data: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-file",
   "metadata": {
    "id": "upload-file"
   },
   "outputs": [],
   "source": [
    "# Upload the training file\n",
    "with open(temp_file, 'rb') as f:\n",
    "    file_response = client.files.create(\n",
    "        file=f,\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "\n",
    "print(f\"üìÅ File uploaded: {file_response.id}\")\n",
    "training_file_id = file_response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-training",
   "metadata": {
    "id": "start-training"
   },
   "source": [
    "## üéØ Start Fine-tuning Job\n",
    "\n",
    "Now let's start a supervised fine-tuning (SFT) job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-job",
   "metadata": {
    "id": "create-job"
   },
   "outputs": [],
   "source": [
    "# Create fine-tuning job\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    method={\"type\": \"sft\"},\n",
    "    training_file=training_file_id,\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\"  # Small model for quick training\n",
    ")\n",
    "\n",
    "print(f\"üéØ Training job created: {job.id}\")\n",
    "print(f\"Status: {job.status}\")\n",
    "job_id = job.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-training",
   "metadata": {
    "id": "monitor-training"
   },
   "source": [
    "## üìä Monitor Training Progress with Real-time Logs\n",
    "\n",
    "Arbor now provides built-in log monitoring! You can watch training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-status",
   "metadata": {
    "id": "check-status"
   },
   "outputs": [],
   "source": [
    "# Option 1: Watch job with real-time logs (recommended!)\n",
    "print(\"üé¨ Starting real-time job monitoring...\")\n",
    "arbor.watch_job(job_id, max_time=300, show_logs=True)  # Watch for 5 minutes max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nv22707rj4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Traditional polling approach\n",
    "import time\n",
    "\n",
    "# Check job status manually\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(f\"üìä Job Status: {job_status.status}\")\n",
    "\n",
    "# Monitor until completion (with timeout)\n",
    "timeout = 300  # 5 minutes\n",
    "start_time = time.time()\n",
    "\n",
    "while job_status.status in ['queued', 'running'] and (time.time() - start_time) < timeout:\n",
    "    time.sleep(10)  # Check every 10 seconds\n",
    "    job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    print(f\"‚è±Ô∏è  [{elapsed}s] Status: {job_status.status}\")\n",
    "\n",
    "print(f\"\\nüéâ Final Status: {job_status.status}\")\n",
    "if job_status.fine_tuned_model:\n",
    "    print(f\"‚úÖ Fine-tuned model: {job_status.fine_tuned_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vbh7habr84j",
   "metadata": {},
   "source": [
    "### üÜö Monitoring Options\n",
    "\n",
    "**Option 1: `arbor.watch_job()`** - Shows real-time training logs as they happen ‚ú®  \n",
    "**Option 2: Traditional polling** - Just checks status periodically\n",
    "\n",
    "The new `watch_job()` function gives you much better visibility into what's happening during training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-finetuned",
   "metadata": {
    "id": "test-finetuned"
   },
   "source": [
    "## üß™ Test Fine-tuned Model\n",
    "\n",
    "If training completed successfully, let's test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-finetuned-model",
   "metadata": {
    "id": "test-finetuned-model"
   },
   "outputs": [],
   "source": [
    "if job_status.status == 'succeeded' and job_status.fine_tuned_model:\n",
    "    # Test the fine-tuned model\n",
    "    response = client.chat.completions.create(\n",
    "        model=job_status.fine_tuned_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is reinforcement learning?\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(\"üé≠ Fine-tuned model response:\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Training not completed yet. Status: {job_status.status}\")\n",
    "    if job_status.status == 'failed':\n",
    "        print(\"‚ùå Training failed. Check the logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-example",
   "metadata": {
    "id": "advanced-example"
   },
   "source": [
    "## üöÄ Advanced: GRPO Training\n",
    "\n",
    "For more advanced users, here's how to do GRPO (Generalized Reward-based Policy Optimization) training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-example",
   "metadata": {
    "id": "grpo-example"
   },
   "outputs": [],
   "source": [
    "# GRPO training example (commented out by default)\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# grpo_job = client.fine_tuning.jobs.create(\n",
    "#     method={\n",
    "#         \"type\": \"grpo\",\n",
    "#         \"hyperparameters\": {\n",
    "#             \"num_train_epochs\": 1,\n",
    "#             \"learning_rate\": 5e-5,\n",
    "#             \"per_device_train_batch_size\": 1\n",
    "#         }\n",
    "#     },\n",
    "#     training_file=training_file_id,\n",
    "#     model=\"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "# )\n",
    "# \n",
    "# print(f\"üéØ GRPO job created: {grpo_job.id}\")\n",
    "\n",
    "print(\"üí° GRPO example is commented out. Uncomment to try it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server-management",
   "metadata": {
    "id": "server-management"
   },
   "source": [
    "## üõ†Ô∏è Server Management\n",
    "\n",
    "Arbor provides utilities to check server status and manage the background process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server-status",
   "metadata": {
    "id": "server-status"
   },
   "outputs": [],
   "source": [
    "# Check server status\n",
    "status = arbor.status()\n",
    "if status:\n",
    "    print(\"üå≥ Arbor server is running:\")\n",
    "    print(f\"   Host: {status['host']}\")\n",
    "    print(f\"   Port: {status['port']}\")\n",
    "    print(f\"   Base URL: {status['base_url']}\")\n",
    "    print(f\"   Storage: {status['storage_path']}\")\n",
    "    print(f\"   GPU IDs: {status['gpu_ids']}\")\n",
    "else:\n",
    "    print(\"‚ùå No Arbor server running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## üßπ Cleanup\n",
    "\n",
    "When you're done, you can shut down the Arbor server (optional - it will auto-cleanup when the notebook ends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown",
   "metadata": {
    "id": "shutdown"
   },
   "outputs": [],
   "source": [
    "# Shutdown the server (optional)\n",
    "# arbor.shutdown()\n",
    "\n",
    "# Clean up temporary file\n",
    "if 'temp_file' in locals():\n",
    "    os.unlink(temp_file)\n",
    "    print(\"üóëÔ∏è  Cleaned up temporary training file\")\n",
    "\n",
    "print(\"‚úÖ Session cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that you've got Arbor running in Colab, here are some things to try:\n",
    "\n",
    "1. **Experiment with different models**: Try larger models like `Qwen/Qwen2-0.5B-Instruct`\n",
    "2. **Use your own data**: Upload your own JSONL training files\n",
    "3. **Try GRPO training**: Uncomment the GRPO example above\n",
    "4. **Integrate with DSPy**: Use Arbor as a backend for DSPy programs\n",
    "5. **Monitor training**: Use the monitoring endpoints to track GPU usage and training metrics\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Arbor GitHub Repository](https://github.com/Ziems/arbor)\n",
    "- [DSPy RL Optimization Examples](https://dspy.ai/tutorials/rl_papillon/)\n",
    "- [DSPy Discord Community](https://discord.gg/ZAEGgxjPUe)\n",
    "\n",
    "### üí° Pro Tips for Colab\n",
    "\n",
    "- Use `arbor.init()` at the start of your notebook - it's designed to be safe to call multiple times\n",
    "- The server will auto-cleanup when your Colab session ends\n",
    "- For long-running training jobs, consider upgrading to Colab Pro for more stable GPU access\n",
    "- Monitor your GPU usage to avoid hitting Colab limits\n",
    "\n",
    "Happy fine-tuning! üå≥‚ú®"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
