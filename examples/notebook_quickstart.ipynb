{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-header",
   "metadata": {
    "id": "colab-header"
   },
   "source": [
    "# üå≥ Arbor Notebook Demo - Quick Start\n",
    "\n",
    "This notebook demonstrates how to use Arbor in Jupyter notebooks for fine-tuning language models with a Ray-like interface.\n",
    "\n",
    "**Works in:**\n",
    "- üìì Local Jupyter Lab/Notebook\n",
    "- ‚òÅÔ∏è Google Colab \n",
    "- üß™ VS Code Notebooks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ziems/arbor/blob/main/examples/notebook_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "First, let's install Arbor using `uv` for faster package management. This may take a few minutes as it installs the ML dependencies, but uv is significantly faster than pip.\n",
    "\n",
    "**Note:** If you don't have `uv` installed locally, you can use `pip` instead by replacing `uv pip install --system` with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-arbor",
   "metadata": {
    "id": "install-arbor"
   },
   "outputs": [],
   "source": [
    "# Install uv for faster package management\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Install Arbor from the colab-support branch (includes new Colab integration)\n",
    "!uv pip install --system git+https://github.com/Ziems/arbor.git@main git+https://github.com/stanfordnlp/dspy.git@main openai \n",
    "\n",
    "# Optional: Install flash attention for faster inference (takes ~15 min)\n",
    "# !uv pip install --system flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialization",
   "metadata": {
    "id": "initialization"
   },
   "source": [
    "## üöÄ Start Arbor Server\n",
    "\n",
    "Start Arbor in the background. This will:\n",
    "- Auto-detect available GPUs\n",
    "- Create a config file automatically\n",
    "- Start the server in a background thread\n",
    "- Provide an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-arbor",
   "metadata": {
    "id": "init-arbor"
   },
   "outputs": [],
   "source": [
    "import arbor\n",
    "\n",
    "# Start Arbor server in the background\n",
    "server_info = arbor.init()\n",
    "print(f\"Server running at: {server_info['base_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-setup",
   "metadata": {
    "id": "client-setup"
   },
   "source": [
    "## üîå Connect with OpenAI Client\n",
    "\n",
    "Use the standard OpenAI Python client to interact with Arbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-client",
   "metadata": {
    "id": "setup-client"
   },
   "outputs": [],
   "source": [
    "# Option 1: Use the convenience method\n",
    "client = arbor.get_client()\n",
    "\n",
    "# Option 2: Manual setup\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=server_info['base_url'],\n",
    "#     api_key=\"not-needed\"  # Local server doesn't need API key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-inference",
   "metadata": {
    "id": "test-inference"
   },
   "source": [
    "## üí¨ Test Inference\n",
    "\n",
    "Let's test inference with a small model to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-chat",
   "metadata": {
    "id": "test-chat"
   },
   "outputs": [],
   "source": [
    "# Test chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\",  # Small model for testing\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Tell me a joke about machine learning.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b0638",
   "metadata": {},
   "source": [
    "The inference job for that model will persist as long as we want, but we won't be doing inference with that model anymore so let's go ahead and shut that job down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbor.shutdown_job(\"HuggingFaceTB/SmolLM2-135M-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-data",
   "metadata": {
    "id": "upload-data"
   },
   "source": [
    "## üìÅ Upload Training Data\n",
    "\n",
    "Let's create some sample training data and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-data",
   "metadata": {
    "id": "create-data"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create sample SFT training data\n",
    "sample_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explain neural networks simply.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process information and learn patterns from data.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Deep learning is a type of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "    temp_file = f.name\n",
    "\n",
    "print(f\"Created training data: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-file",
   "metadata": {
    "id": "upload-file"
   },
   "outputs": [],
   "source": [
    "# Upload the training file\n",
    "with open(temp_file, 'rb') as f:\n",
    "    file_response = client.files.create(\n",
    "        file=f,\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "\n",
    "print(f\"üìÅ File uploaded: {file_response.id}\")\n",
    "training_file_id = file_response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-training",
   "metadata": {
    "id": "start-training"
   },
   "source": [
    "## üéØ Start Fine-tuning Job\n",
    "\n",
    "Now let's start a supervised fine-tuning (SFT) job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-job",
   "metadata": {
    "id": "create-job"
   },
   "outputs": [],
   "source": [
    "# Create fine-tuning job\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    method={\"type\": \"sft\"},\n",
    "    training_file=training_file_id,\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\"  # Small model for quick training\n",
    ")\n",
    "\n",
    "print(f\"üéØ Training job created: {job.id}\")\n",
    "print(f\"Status: {job.status}\")\n",
    "job_id = job.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-training",
   "metadata": {
    "id": "monitor-training"
   },
   "source": [
    "## üìä Monitor Training Progress with Real-time Logs\n",
    "\n",
    "Arbor now provides built-in log monitoring! You can watch training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-status",
   "metadata": {
    "id": "check-status"
   },
   "outputs": [],
   "source": [
    "# Option 1: Watch job with real-time logs (recommended!)\n",
    "print(\"üé¨ Starting real-time job monitoring...\")\n",
    "arbor.watch_job(job_id, max_time=300, show_logs=True)  # Watch for 5 minutes max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nv22707rj4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Traditional polling approach\n",
    "import time\n",
    "\n",
    "# Check job status manually\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(f\"üìä Job Status: {job_status.status}\")\n",
    "\n",
    "# Monitor until completion (with timeout)\n",
    "timeout = 300  # 5 minutes\n",
    "start_time = time.time()\n",
    "\n",
    "while job_status.status in ['queued', 'running'] and (time.time() - start_time) < timeout:\n",
    "    time.sleep(10)  # Check every 10 seconds\n",
    "    job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    print(f\"‚è±Ô∏è  [{elapsed}s] Status: {job_status.status}\")\n",
    "\n",
    "print(f\"\\nüéâ Final Status: {job_status.status}\")\n",
    "if job_status.fine_tuned_model:\n",
    "    print(f\"‚úÖ Fine-tuned model: {job_status.fine_tuned_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vbh7habr84j",
   "metadata": {},
   "source": [
    "### üÜö Monitoring Options\n",
    "\n",
    "**Option 1: `arbor.watch_job()`** - Shows real-time training logs as they happen ‚ú®  \n",
    "**Option 2: Traditional polling** - Just checks status periodically\n",
    "\n",
    "The new `watch_job()` function gives you much better visibility into what's happening during training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-finetuned",
   "metadata": {
    "id": "test-finetuned"
   },
   "source": [
    "## üß™ Test Fine-tuned Model\n",
    "\n",
    "If training completed successfully, let's test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-finetuned-model",
   "metadata": {
    "id": "test-finetuned-model"
   },
   "outputs": [],
   "source": [
    "if job_status.status == 'succeeded' and job_status.fine_tuned_model:\n",
    "    # Test the fine-tuned model\n",
    "    response = client.chat.completions.create(\n",
    "        model=job_status.fine_tuned_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is reinforcement learning?\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(\"üé≠ Fine-tuned model response:\")\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "    # Shutdown the inference job\n",
    "    arbor.shutdown_job(job_status.fine_tuned_model)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Training not completed yet. Status: {job_status.status}\")\n",
    "    if job_status.status == 'failed':\n",
    "        print(\"‚ùå Training failed. Check the logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-example",
   "metadata": {
    "id": "advanced-example"
   },
   "source": [
    "## üöÄ Advanced: DSPy GRPO with Multi-GPU Configuration\n",
    "\n",
    "DSPy's GRPO optimizer requires a multi-GPU setup where inference and training run on separate GPUs for optimal performance. This requires at least 2 GPUs.\n",
    "\n",
    "**Note**: Standard Google Colab provides only 1 GPU, so you'll need:\n",
    "- A local machine with multiple GPUs, or\n",
    "- A cloud instance with multiple GPUs\n",
    "\n",
    "For single GPU environments, stick with the SFT training shown above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-example",
   "metadata": {
    "id": "grpo-example"
   },
   "outputs": [],
   "source": [
    "# DSPy GRPO with GPU Memory Sharing Example\n",
    "import dspy\n",
    "import random\n",
    "\n",
    "# Sample classification data (banking domain)\n",
    "sample_classification_data = [\n",
    "    dspy.Example(text=\"I want to transfer money to my friend\", label=\"transfer\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"What is my account balance?\", label=\"balance\").with_inputs(\"text\"), \n",
    "    dspy.Example(text=\"I lost my credit card\", label=\"card_issues\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"How do I reset my PIN?\", label=\"pin_change\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"When will my salary be deposited?\", label=\"direct_deposit\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"I need to pay my bills\", label=\"bill_payment\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"Can I increase my credit limit?\", label=\"credit_limit\").with_inputs(\"text\"),\n",
    "    dspy.Example(text=\"Where is the nearest ATM?\", label=\"atm_location\").with_inputs(\"text\"),\n",
    "]\n",
    "\n",
    "# Create train and validation sets\n",
    "random.Random(42).shuffle(sample_classification_data)\n",
    "trainset = sample_classification_data[:6]\n",
    "valset = sample_classification_data[6:]\n",
    "\n",
    "print(f\"üìä Created training set: {len(trainset)} examples\")\n",
    "print(f\"üìä Created validation set: {len(valset)} examples\")\n",
    "\n",
    "# Define the classification task\n",
    "CLASSES = [\"transfer\", \"balance\", \"card_issues\", \"pin_change\", \"direct_deposit\", \"bill_payment\", \"credit_limit\", \"atm_location\"]\n",
    "classify = dspy.ChainOfThought(f\"text -> label: Literal{CLASSES}\")\n",
    "\n",
    "# Set up DSPy with Arbor provider\n",
    "from dspy.clients.lm_local_arbor import ArborProvider\n",
    "\n",
    "provider = ArborProvider()\n",
    "student_lm = dspy.LM(\n",
    "    model=f\"openai/arbor:Qwen/Qwen2-0.5B-Instruct\",  # Use the same small model\n",
    "    provider=provider, \n",
    "    temperature=0.7, \n",
    "    api_base=server_info['base_url'], \n",
    "    api_key=\"arbor\"\n",
    ")\n",
    "\n",
    "# Create student classifier\n",
    "student_classify = classify.deepcopy()\n",
    "student_classify.set_lm(student_lm)\n",
    "\n",
    "# Define evaluation metric\n",
    "metric = (lambda x, y, trace=None: x.label == y.label)\n",
    "\n",
    "print(f\"üéØ Classification task: {len(CLASSES)} classes\")\n",
    "print(\"üîß DSPy classifier set up with Arbor backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3yot6mruc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GRPO optimizer with multi-GPU setup\n",
    "from dspy.teleprompt.grpo import GRPO\n",
    "\n",
    "# Training configuration with multi-GPU setup\n",
    "train_kwargs = {\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'temperature': 0.7,\n",
    "    'beta': 0.02,\n",
    "    'learning_rate': 1e-5,\n",
    "    'gradient_checkpointing': False,\n",
    "    'bf16': True,\n",
    "    'lr_scheduler_type': 'constant_with_warmup',\n",
    "    'max_prompt_length': None,\n",
    "    'max_completion_length': None,\n",
    "    'lora': False,\n",
    "    'report_to': 'none',\n",
    "    'log_completions': False,\n",
    "}\n",
    "\n",
    "# Create GRPO compiler\n",
    "compiler = GRPO(\n",
    "    metric=metric,\n",
    "    multitask=True,\n",
    "    num_dspy_examples_per_grpo_step=2,\n",
    "    num_rollouts_per_grpo_step=2,\n",
    "    exclude_demos=True,\n",
    "    num_train_steps=10,\n",
    "    num_threads=4,\n",
    "    use_train_as_val=False,\n",
    "    num_steps_for_val=5,\n",
    "    train_kwargs=train_kwargs,\n",
    ")\n",
    "\n",
    "print(\"üöÄ GRPO compiler configured with multi-GPU setup\")\n",
    "print(\"   - Type: Multi-GPU configuration\")\n",
    "print(\"   - Inference GPUs: 1 (runs VLLM server)\")\n",
    "print(\"   - Training GPUs: 1 (runs training process)\")\n",
    "print(\"   - Total GPUs required: 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ikidoy0z3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GRPO optimization (this will take several minutes)\n",
    "print(\"üèÉ Starting GRPO optimization...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. Launch VLLM inference server on GPU 0\")\n",
    "print(\"  2. Start training process on GPU 1\") \n",
    "print(\"  3. Optimize the classifier using GRPO learning\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è  REQUIREMENTS:\")\n",
    "print(\"   - At least 2 GPUs available\")\n",
    "print(\"   - Standard Colab (1 GPU) will NOT work\")\n",
    "print(\"   - Use Colab Pro+, local multi-GPU, or cloud instance\")\n",
    "print(\"\")\n",
    "\n",
    "# Check if we have enough GPUs\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"üîç Detected {gpu_count} GPU(s)\")\n",
    "    \n",
    "    if gpu_count < 2:\n",
    "        print(\"‚ùå GRPO requires at least 2 GPUs\")\n",
    "        print(\"   Skipping GRPO demonstration\")\n",
    "    else:\n",
    "        print(\"‚úÖ Sufficient GPUs detected for GRPO\")\n",
    "        \n",
    "        # Compile the classifier with GRPO\n",
    "        classify_optimized = compiler.compile(\n",
    "            student=student_classify,\n",
    "            trainset=trainset,\n",
    "            valset=valset,\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ GRPO optimization completed!\")\n",
    "        print(\"üìà Your classifier has been optimized using reinforcement learning\")\n",
    "else:\n",
    "    print(\"‚ùå No CUDA GPUs detected\")\n",
    "    print(\"   GRPO requires GPU acceleration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server-management",
   "metadata": {
    "id": "server-management"
   },
   "source": [
    "## üõ†Ô∏è Server Management\n",
    "\n",
    "Arbor provides utilities to check server status and manage the background process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server-status",
   "metadata": {
    "id": "server-status"
   },
   "outputs": [],
   "source": [
    "# Check server status\n",
    "status = arbor.status()\n",
    "if status:\n",
    "    print(\"üå≥ Arbor server is running:\")\n",
    "    print(f\"   Host: {status['host']}\")\n",
    "    print(f\"   Port: {status['port']}\")\n",
    "    print(f\"   Base URL: {status['base_url']}\")\n",
    "    print(f\"   Storage: {status['storage_path']}\")\n",
    "    print(f\"   GPU IDs: {status['gpu_ids']}\")\n",
    "else:\n",
    "    print(\"‚ùå No Arbor server running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that you've got Arbor running, here are some things to try:\n",
    "\n",
    "1. **Experiment with different models**: Try larger models like `Qwen/Qwen2-0.5B-Instruct`\n",
    "2. **Use your own data**: Upload your own JSONL training files\n",
    "3. **Try GRPO training**: Use the GRPO example above if you have multiple GPUs\n",
    "4. **Integrate with DSPy**: Use Arbor as a backend for DSPy programs\n",
    "5. **Monitor training**: Use the monitoring endpoints to track GPU usage and training metrics\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Arbor GitHub Repository](https://github.com/Ziems/arbor)\n",
    "- [DSPy RL Optimization Examples](https://dspy.ai/tutorials/rl_papillon/)\n",
    "- [DSPy Discord Community](https://discord.gg/ZAEGgxjPUe)\n",
    "\n",
    "### üí° Pro Tips\n",
    "\n",
    "**For Single GPU (Standard Colab):**\n",
    "- Use `arbor.init()` at the start of your notebook - it's designed to be safe to call multiple times\n",
    "- SFT training works great with single GPU\n",
    "- The server will auto-cleanup when your session ends\n",
    "- Use `arbor.shutdown_job()` to free up GPU resources when switching models\n",
    "\n",
    "**For Multi-GPU (Colab Pro+, Local, Cloud):**\n",
    "- GRPO training requires at least 2 GPUs (1 for inference, 1 for training)\n",
    "- Use the advanced GRPO examples for reinforcement learning from preferences\n",
    "- Monitor your GPU usage to optimize resource allocation\n",
    "- Consider larger models and batch sizes with more GPU memory\n",
    "\n",
    "### üîß GPU Requirements Summary\n",
    "\n",
    "| Feature | GPU Requirement | Environment |\n",
    "|---------|----------------|-------------|\n",
    "| **Standard Training** | 1 GPU | ‚úÖ Colab, Local, Cloud |\n",
    "| **Model Inference** | 1 GPU | ‚úÖ Colab, Local, Cloud |\n",
    "| **GRPO Training** | 2+ GPUs | ‚ùå Colab, Local, Cloud |\n",
    "\n",
    "Happy fine-tuning! üå≥‚ú®"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
